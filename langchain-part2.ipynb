{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Bootstrapping for Credentials"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cdaab290130cdea8"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dotenv\n",
    "dotenv.load_dotenv()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:01.506589Z",
     "start_time": "2024-01-20T21:57:01.398032Z"
    }
   },
   "id": "b5980e2be0c631be"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Simple parameterized LLM call\n",
    "\n",
    "Model + Prompt"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7993353448bffbe"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Note: langchain recently refactored their package structure\n",
    "# As of langchain 0.1 LLM provider specific abstractions live in\n",
    "# dedicated packages that follow the <langchain_provider> format\n",
    "\n",
    "# from langchain.chat_models import ChatOpenAI # legacy\n",
    "from langchain_openai import ChatOpenAI # new way\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:02.022856Z",
     "start_time": "2024-01-20T21:57:01.508440Z"
    }
   },
   "id": "2740841090717a0c"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "model = ChatOpenAI(\n",
    "  model=\"gpt-4-1106-preview\"\n",
    "  # model=\"gpt-4\"\n",
    "  # model=\"gpt-3.5-turbo\"\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:02.057314Z",
     "start_time": "2024-01-20T21:57:02.025308Z"
    }
   },
   "id": "9b6addf6e55ec433"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"Why do Pythons live on land?\\n\\nBecause they're above C-level.\")"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(\"Tell me an intelligent yet genuinely funny joke about python\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:03.309272Z",
     "start_time": "2024-01-20T21:57:02.059346Z"
    }
   },
   "id": "f33e5e3c0a5b44bd"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"Tell me an intelligent yet genuinely funny joke about {topic}\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:03.317570Z",
     "start_time": "2024-01-20T21:57:03.312597Z"
    }
   },
   "id": "7470b060274c6563"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "ChatPromptValue(messages=[HumanMessage(content='Tell me an intelligent yet genuinely funny joke about lions')])"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt.invoke({\"topic\": \"lions\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:03.320991Z",
     "start_time": "2024-01-20T21:57:03.317267Z"
    }
   },
   "id": "f49c96e70e1d173f"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Important insight:\n",
    "\n",
    "All the essential building blocks provided by LangChain (models, prompts, retrievers) share a common interface :-)\n",
    "\n",
    "They all support, amongst others:\n",
    "```\n",
    "# invoke\n",
    "# stream\n",
    "# batch\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9cab692b42e52df2"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Knowing that both the _model_, and the _prompt_ support being \"invoked\", we can do this:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "886907e02d401c1e"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content=\"Why don't cats play poker in the jungle?\\n\\nToo many cheetahs.\")"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.invoke(prompt.invoke({\"topic\": \"cats\"}))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:05.833937Z",
     "start_time": "2024-01-20T21:57:03.323139Z"
    }
   },
   "id": "d9c0ae5f432a39b9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "But there's a much more elegant way..."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "dd5a64e033029d87"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LangChain Expression Language (LCEL)\n",
    "\n",
    "LCEL allows us to pipe together multiple of the elemental building blocks into a single object, a so-called _chain_. The resulting chain can then also be invoked, streamed, or batched, analogously to the individual building blocks it is composed of."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2ced36a9da6bc485"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "chain = prompt | model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:05.838308Z",
     "start_time": "2024-01-20T21:57:05.834102Z"
    }
   },
   "id": "968927b94778c72b"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "AIMessage(content='A snail walks into a car dealership and buys a sleek new sports car. He asks the salesman to have a special modification made: he wants a big \"S\" painted on each side, the roof, and the trunk of the car.\\n\\nPuzzled, the salesman asks why the snail wants this customization.\\n\\nThe snail responds, \"When I zoom past people on the highway, I want them to say, \\'Look at that S-car go!\\'\"')"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"snail\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:11.347603Z",
     "start_time": "2024-01-20T21:57:05.837853Z"
    }
   },
   "id": "d5425ea0cbaa139"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's add another building block to the chain: an output parser"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b751d0c6b4789fee"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:11.360972Z",
     "start_time": "2024-01-20T21:57:11.346894Z"
    }
   },
   "id": "2a1b30a495472a8a"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "chain = prompt | model | output_parser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:11.386152Z",
     "start_time": "2024-01-20T21:57:11.364034Z"
    }
   },
   "id": "a777b54dc5da8736"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "'Why do Python programmers prefer dark mode?\\n\\nBecause light attracts bugs.'"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.invoke({\"topic\": \"python\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:12.724583Z",
     "start_time": "2024-01-20T21:57:11.365656Z"
    }
   },
   "id": "3f11fb1cffc0b79c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### How can we invoke the chain with a string as input, rather than a full dictionary?"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7ddcd7a13ea0af7a"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected mapping type as input to ChatPromptTemplate. Received <class 'str'>.\n"
     ]
    }
   ],
   "source": [
    "# This will fail:\n",
    "try:\n",
    "  chain.invoke(\"lemons\")\n",
    "except TypeError as err:\n",
    "  print(err)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:12.727157Z",
     "start_time": "2024-01-20T21:57:12.724684Z"
    }
   },
   "id": "b8346fb9a573127d"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can use the special `RunnablePassthrough` to maintain the input value but map it to the expected key-value format:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9128c8217e5e8d22"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "chain_str = {\"topic\": RunnablePassthrough()} | chain"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:12.730199Z",
     "start_time": "2024-01-20T21:57:12.727356Z"
    }
   },
   "id": "ff6fcd6054769a7"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "'Why did the lemon stop halfway across the road?\\n\\nBecause it ran out of juice!'"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_str.invoke(\"lemons\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:13.826958Z",
     "start_time": "2024-01-20T21:57:12.729830Z"
    }
   },
   "id": "67e784790ca70d1d"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "4d28a755590b125b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Parametrization with more than one parameter"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "418aa2f88844cfc3"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Tell me a joke about {topic} \n",
    "\n",
    "Answer in {language}\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:13.830656Z",
     "start_time": "2024-01-20T21:57:13.827361Z"
    }
   },
   "id": "2e3f41320ffade30"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "chain_with_multiple_parameters = prompt | model | output_parser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:13.833708Z",
     "start_time": "2024-01-20T21:57:13.830310Z"
    }
   },
   "id": "2c1525cd49f4b86"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "'Warum mögen Python-Programmierer keine Umlaute?\\n\\nWeil sie ständig mit Encodings kämpfen müssen!'"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_multiple_parameters.invoke({\"topic\": \"python\", \"language\": \"German\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:16.537965Z",
     "start_time": "2024-01-20T21:57:13.836203Z"
    }
   },
   "id": "8feb5cc50e2f046"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This is a good example to illustrate the power of the `.batch` method that all LCEL objects support. This way multiple calls can be made in parallel:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "6d75cc795f42c100"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['Why did the lemon stop rolling down the hill?\\n\\nBecause it ran out of juice!',\n 'Warum mögen Pythons keine Umlaute?\\n\\nWeil sie beim Versuch, \"Schönheit\" zu sagen, immer eine Exception werfen!',\n 'Waarom zijn katten slecht in videogames?\\n\\nOmdat ze altijd met de muis spelen!']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_with_multiple_parameters.batch([{\"topic\": \"lemons\", \"language\": \"English\"},\n",
    "                                      {\"topic\": \"python\", \"language\": \"German\"},\n",
    "                                      {\"topic\": \"cats\", \"language\": \"Dutch\"}\n",
    "                                      ])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:17.977646Z",
     "start_time": "2024-01-20T21:57:16.539792Z"
    }
   },
   "id": "a62df5a882fc397a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Stream the output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2d38fcfbf56e1b64"
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Write a 100 word love-letter about Python.\n",
    "\"\"\")\n",
    "\n",
    "chain_long_answer =  prompt | model | output_parser"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:17.982198Z",
     "start_time": "2024-01-20T21:57:17.978059Z"
    }
   },
   "id": "4a5eb08d8ceebf36"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Dearest Python,\n",
      "\n",
      "From the first moment I beheld your elegant syntax, my heart was enraptured. You’ve charmed me with your simplicity and ensnared me with your versatility. Each line of your code is a love note, transforming my thoughts into action with effortless grace. Your libraries, like bouquets of functionality, bloom with possibility. You've taught me the true meaning of efficiency, allowing our dance of logic to flow like prose.\n",
      "\n",
      "In your embrace, I’ve found a sanctuary of creativity where I can express myself with clarity and purpose. My beloved Python, you are the serenade to my algorithmic soul.\n",
      "\n",
      "Yours in code,\n",
      "A smitten developer"
     ]
    }
   ],
   "source": [
    "for token in chain_long_answer.stream({}):\n",
    "  print(token, end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:24.952520Z",
     "start_time": "2024-01-20T21:57:17.984831Z"
    }
   },
   "id": "f633d125c714cf32"
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Local RAG – Sample Implementation"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "ce2bb69c36a10ef1"
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# from langchain.embeddings import OpenAIEmbeddings # legacy\n",
    "from langchain_openai import OpenAIEmbeddings # new way\n",
    "\n",
    "from langchain.vectorstores import Chroma"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:24.962386Z",
     "start_time": "2024-01-20T21:57:24.954738Z"
    }
   },
   "id": "a3ff2410c505fe38"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let's define some facts and store them in a vectorstore:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2f5774cbcc375ab9"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "vectorstore = Chroma.from_texts([\n",
    "  \"Arno likes walks in nature\",\n",
    "  \"Bob loves Python\",\n",
    "  \"Mira enjoys working with Prefect\",\n",
    "  \"Craig is a big fan of cycling\",\n",
    "],\n",
    "embedding=OpenAIEmbeddings()\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:25.816885Z",
     "start_time": "2024-01-20T21:57:24.964251Z"
    }
   },
   "id": "be5245a80950a71d"
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:25.819256Z",
     "start_time": "2024-01-20T21:57:25.817582Z"
    }
   },
   "id": "9588008f04e7874b"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can retrieve the closest fact to a given query:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1b35d4f80506c01"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='Bob loves Python')]"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"What does Bob like?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:26.062705Z",
     "start_time": "2024-01-20T21:57:25.821859Z"
    }
   },
   "id": "b9e756e51e916cf7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The embeddings encapsulate \"meaning\", thus we can also retrieve facts that are semantically close to the query:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f7a18b09f95fcd67"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='Craig is a big fan of cycling')]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever.invoke(\"Who has a hobby that involves two wheels?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:26.265708Z",
     "start_time": "2024-01-20T21:57:26.066167Z"
    }
   },
   "id": "37559a9af5526f19"
  },
  {
   "cell_type": "markdown",
   "source": [
    "The embeddings can also infer semantic similarity across languages:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d81af64ad59c5938"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "[Document(page_content='Arno likes walks in nature')]"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The question is in German: \"Who likes to take walks?\"\n",
    "retriever.invoke(\"Wer mag Spaziergänge?\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:26.497560Z",
     "start_time": "2024-01-20T21:57:26.265491Z"
    }
   },
   "id": "99f0396364e41d3c"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we can combine the retriever with a prompt and model to create an LLM that is grounded in our custom knowledge base:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "4870fb8e8bb4d36c"
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Optional: Enable debug mode to see the full chain of operations\n",
    "# from langchain.globals import set_debug\n",
    "# set_debug(False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:26.499628Z",
     "start_time": "2024-01-20T21:57:26.498216Z"
    }
   },
   "id": "591153e83bf44029"
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:26.505865Z",
     "start_time": "2024-01-20T21:57:26.504420Z"
    }
   },
   "id": "ec71f3c4c55d3ff9"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\"\"\"\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question:\n",
    "{question}\n",
    "\"\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:26.507812Z",
     "start_time": "2024-01-20T21:57:26.506797Z"
    }
   },
   "id": "724f64019ca1fde7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Our end goals is to be able to invoke the chain with a question like this:\n",
    "`chain.invoke({\"question\": \"What do you know about Mira?\"})`\n",
    "\n",
    "We can compose a chain that allows us to do this as follows:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bc8165e48a61fc3"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "chain_rag = (\n",
    "        {\n",
    "          \"context\": itemgetter(\"question\") | retriever,\n",
    "          \"question\": itemgetter(\"question\"),\n",
    "        }\n",
    "\n",
    "  | prompt\n",
    "  | model\n",
    "  | output_parser\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:26.510767Z",
     "start_time": "2024-01-20T21:57:26.509388Z"
    }
   },
   "id": "fb6f859db7affadb"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Explanation:\n",
    "The outermost `(` and `)` are simply there to allow us to freely add line-breaks for easier readability.\n",
    "\n",
    "The final composition of `prompt`, `model`, and `output_parser` are the same as before.\n",
    "\n",
    "The only difference in the chain definition is the addition of the dictionary at the beginning of the chain, which is used to determine the value of the two keys: `context` and `question` which are expected as input to the prompt we defined above. The values of the dictionary can be LCEL objects themselves, as is the case for the `retriever` in this example.\n",
    "\n",
    "The usage of `itemgetter` allows us to extract the value of a key from a dictionary. In this case, the chain will be called with a single attribute as input, namely `question`. The `itemgetter` will extract the value of the key `question` from the input dictionary twice. Once to pass it to the retriever to obtain the semantically most relevant facts and store them as the `context`. And a second time to preserve the value of the question and make it available as input to the next step in the chain, the prompt."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2797c5b3bbc11acb"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the provided context, I know that Mira enjoys working with something called Prefect. However, without additional information, it's unclear whether Prefect refers to a person, a job title, a company, a software tool, or something else entirely. The context does not provide any further details about Mira, such as her profession, background, or the nature of her work with Prefect."
     ]
    }
   ],
   "source": [
    "for token in chain_rag.stream({\"question\": \"What do you know about Mira?\"}):\n",
    "  print(token, end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:31.147285Z",
     "start_time": "2024-01-20T21:57:26.512597Z"
    }
   },
   "id": "9599b027735363c9"
  },
  {
   "cell_type": "markdown",
   "source": [
    "This works as intended :-)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41e5db077ccf7d1a"
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can apply the same trick as before to allow the chain to be invoked with a string as input:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8cbef8a9702b2e90"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:31.149288Z",
     "start_time": "2024-01-20T21:57:31.148093Z"
    }
   },
   "id": "50f1b377a6ec0d93"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "chain_rag_v2 = (\n",
    "    {\"question\": RunnablePassthrough()}\n",
    "  | chain_rag\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:31.151858Z",
     "start_time": "2024-01-20T21:57:31.150407Z"
    }
   },
   "id": "2bd1084346084c79"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Craig harbors a deep passion for cycling, a sport that captivates his interest and likely consumes much of his free time. He may spend weekends pedaling through scenic trails or attending cycling events. His dedication suggests he could be an advocate for cycling benefits, possibly encouraging others to embrace the activity."
     ]
    }
   ],
   "source": [
    "for token in chain_rag_v2.stream(\"What do you know about Craig? Can you reply with a 50 word essay and make up some more relevant filler information?\"):\n",
    "  print(token, end=\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:36.537475Z",
     "start_time": "2024-01-20T21:57:31.153894Z"
    }
   },
   "id": "758f868d43470179"
  },
  {
   "cell_type": "markdown",
   "source": [
    "If we wish to maintain the context from the retriever throughout the whole chain, we can do that in the following way:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e59bc9b551189cd0"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "chain_rag_with_context = (\n",
    "  RunnablePassthrough() \n",
    "  |\n",
    "        {\n",
    "          \"context\": itemgetter(\"question\") | retriever,\n",
    "          \"question\": itemgetter(\"question\"),\n",
    "        }\n",
    "\n",
    "  |\n",
    "        {\n",
    "          \"answer\": {\"question\": itemgetter(\"question\"),\n",
    "                     \"context\": itemgetter(\"context\")} \n",
    "                    | prompt | model | output_parser,\n",
    "          \"context\": itemgetter(\"context\"),\n",
    "        }\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:36.543541Z",
     "start_time": "2024-01-20T21:57:36.541684Z"
    }
   },
   "id": "6b30082be72c5422"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "data": {
      "text/plain": "{'answer': 'Based on the given context, I know that Arno enjoys taking walks in natural environments.',\n 'context': [Document(page_content='Arno likes walks in nature')]}"
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain_rag_with_context.invoke({\"question\": \"What do you know about Arno?\"})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:38.243898Z",
     "start_time": "2024-01-20T21:57:36.546373Z"
    }
   },
   "id": "2be137313b61a295"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Appendix: Why do we use `itemgetter()`?\n",
    "\n",
    "The `itemgetter()` function is a very simple function that returns a function that can be used to extract the value of a key from a dictionary. It is equivalent to the following lambda function:\n",
    "\n",
    "```python\n",
    "lambda x: x[\"key\"]\n",
    "```\n",
    "\n",
    "Or as a normal function:\n",
    "\n",
    "```python\n",
    "def get_key(x):\n",
    "  return x[\"key\"]\n",
    "```\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1d8e745df9e2d2b1"
  },
  {
   "cell_type": "markdown",
   "source": [
    "In the context of LCEL if we have a chain of the form <step_a> | <step_b> | <step_c>, then the output of <step_a> is passed as input to <step_b>, and the output of <step_b> is passed as input to <step_c>.\n",
    "\n",
    "In the example of `\"context\": itemgetter(\"question\") | retriever,` itemgetter allows us to take the dictionary passed in from the previous step in the chain as input and extract a pre-determined key from it. \n",
    "\n",
    "It allows us to keep the same convention, i.e. the previous steps' output can directly be passed as input to the next step."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "2aa785d24e626422"
  },
  {
   "cell_type": "markdown",
   "source": [
    "Example of itemgetter:"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b00e07c39a6a898e"
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "from operator import itemgetter"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:38.246918Z",
     "start_time": "2024-01-20T21:57:38.243956Z"
    }
   },
   "id": "40114a9f9770cc5b"
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "my_list = [1,2,3]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:38.250170Z",
     "start_time": "2024-01-20T21:57:38.248191Z"
    }
   },
   "id": "3e49482ff42f5476"
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_list[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:38.254416Z",
     "start_time": "2024-01-20T21:57:38.251836Z"
    }
   },
   "id": "fed04d61908c5ec4"
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "getter = itemgetter(0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:38.257751Z",
     "start_time": "2024-01-20T21:57:38.255782Z"
    }
   },
   "id": "d02cb17813cd58cb"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getter(my_list)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:38.278103Z",
     "start_time": "2024-01-20T21:57:38.259302Z"
    }
   },
   "id": "f113b1a67e7b1be"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-20T21:57:38.278806Z",
     "start_time": "2024-01-20T21:57:38.262051Z"
    }
   },
   "id": "c429f373e2599c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
